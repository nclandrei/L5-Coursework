\documentclass[11pt,english,twocolumn]{article}
\renewcommand{\familydefault}{\sfdefault}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{pslatex}
\usepackage[english]{babel}
\usepackage{blindtext}
\usepackage{setspace}
\usepackage{url}
%Definitions from Simon's mya4.sty
% Set the paper size to A4
\setlength{\paperheight}{297mm}
\setlength{\paperwidth}{210mm}
% Define commands which allow the width and height of the text
% to be specified. Centre the text on the page.
\newcommand{\settextwidth}[1]{
\setlength{\textwidth}{#1}
\setlength{\oddsidemargin}{\paperwidth}
\addtolength{\oddsidemargin}{-\textwidth}
\setlength{\oddsidemargin}{0.5\oddsidemargin}
\addtolength{\oddsidemargin}{-1in}
}
\newcommand{\settextheight}[1]{
\setlength{\textheight}{#1}
\setlength{\headheight}{0mm}
\setlength{\headsep}{0mm}
\setlength{\topmargin}{\paperheight}
\addtolength{\topmargin}{-\textheight}
\setlength{\topmargin}{0.5\topmargin}
\addtolength{\topmargin}{-1in}
}
\addtolength{\topsep}{-3mm}% space between first item and preceding paragraph.
\addtolength{\partopsep}{-3mm}% extra space added to \topsep when environment starts a new paragraph.
\addtolength{\itemsep}{-5mm}% space between successive items.

%End of Simon's mya4.sty
\usepackage{graphicx}%This is necessary and it must go after mya4
\settextwidth{176mm}
\settextheight{257mm}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\def\baselinestretch{0.95}

\usepackage[compact]{titlesec}
\titlespacing{\section}{0pt}{*1}{*1}
\titlespacing{\subsection}{0pt}{*1}{*0}
\titlespacing{\subsubsection}{0pt}{*0}{*0}
\titlespacing{\paragraph}{0pt}{*0}{*1}
\titleformat*{\paragraph}{\itshape}{}{}{}
%% --------------------------------------------------------------------------------------------------------------------------------
\begin{document}
\title{Automatic Contextual Bug Report Models}

\author{Andrei-Mihai Nicolae}
\date{}
\maketitle

%====================================================
\section{Introduction}
\label{sec:Introduction}
%====================================================

Software engineering, compared to many other engineering fields, is more
abstract \cite{brooks1995mythical} - in mechanical engineering, for example,
if one wishes to see what is wrong with the engine, the components can be 
touched and manipulated directly by the human operator. However, when taking the example of
compiling and running a simple program written in a language, such as Go 
\cite{golang}, we go through various layers of abstraction which cannot be directly 
seen by the developer: the written characters inside the file get compiled by the
Go compiler to machine readable code; then, the operating system instructs the 
kernel to run the newly generated machine code, which in turn communicates with 
the hardware components responsible for carrying out the necessary computations.

Therefore, having discussed the complex abstraction usually involved in software development,
it is not a trivial task to plan and create applications that fit the requirements of the
stakeholders. Thus, developers have come up with \emph{issue tracking systems}, applications
which help teams plan work, create and assign tasks, monitor progress etc. The basic unit of
functionality these systems make use of is called a \emph{ticket}, which can be usually split
into two main categories: bug reports and feature requests. These software tickets can include 
various types of information, such as:
\begin{itemize}
	\item textual description of the issue/proposed work;
	\item stacktraces that describe the error occured while using the application;
	\item attachments which support the bug report's existence;
	\item story points (i.e. difficulty assigned for that specific ticket, such as 
	how many hours might take a developer to fix it).
\end{itemize}

Usually, for most projects, bug reports are filed on the application's website by ordinary users, many
who do not have technical experience. Thus, a yet unresolved problem arises: how can one
design bug reports that are intuitive and which will produce the most value to developers 
(i.e. help them solve the bug as quickly as possible)? My research tries to find out
what changes need to be brought to modern issue tracking systems so that they can automatically
change the bug report model based on the context (i.e. type of project and underlying technologies).
Thus, a project, such as Mozilla Firefox, which is mainly built using C++, might benefit more from
stacktraces in the bug report rathen than Apache Kafka which is a distributed streaming platform written
in Java, that might benefit more from steps to reproduce from the users.

There are a couple of research papers that are looking at how to improve bug tracking
systems and the bug report models. However, the authors of all of them have either investigated the issue through
qualitative approaches, mainly conducting interviews and questionnaires with developers,
or through analyzing previous pieces of work and inferring possible solutions from there. However,
I believe that these approaches do not yield conclusive results because the developers are biased and
also because simply investigating previous progress on this matter does not bring valuable 
conclusions to the community. Therefore, my research will at first identify what makes for a 
good quality ticket based on the type of project and underlying technologies, and then automatically
re-design the form for filing a bug report using these results.

%-------------------------------------------
\subsection*{Report Structure}
\label{sec:label-subsection}
%-------------------------------------------

This literature review presents the most relevant papers in the fields that surround the
area of software ticket quality. Section \ref{sec:data-quality} looks at data quality and 
metrics in the software development area, such as data quality-related methodologies that 
help organizations develop beneficial data workcycles; Section \ref{sec:ticket-quality} presents 
previous work on defining what are the key elements that make for good quality tickets
in a specific context, while Section \ref{sec:bug-report} presents previous work on
the field that my research is based on (i.e. modelling the bug reports efficiently).
Finally, Section \ref{sec:conclusion} presents conclusions drawn after completing the 
literature survey and possible future directions of the research.

%====================================================
\section{Data Quality and Metrics}
\label{sec:data-quality}
%====================================================

Several authors have investigated the \emph{meaning} of data quality and
what characteristics define it.

Bachman et al.\cite{bachmann2009software} conducted a thorough investigation of several
software projects, both open source as well as closed source, in order 
to infer what determines their quality. They selected 
various sources of information, among which bug tracking 
databases and version control systems logs, and examined SVN and CVS logs,
as well as the content of the bug tracker databases, in the end trying to link 
the logs with the bug tracker contents as they are not integrated by default. 

The study came to several conclusions, among which:
  \begin{itemize}
    \item closed source projects are usually of better quality than open source ones
    (i.e. better average number of attachments, better average number of commits per bug report); 
    one example is the Eclipse open source project which has more than 20\% empty commit messages;
    \item there is reduced traceability between bug reports and version control logs.
  \end{itemize}

A different approach was taken by Strong et al.\cite{strong1997data} who conducted a qualitative analysis
instead of quantitative by collecting data from a large number of data quality projects by 
interviewing custodians, customers and managers. They analyzed each project
using the data quality dimensions as content analysis codes.

Complementing the first study, the authors reached the conclusion that representational data 
quality dimensions are underlying causes of accessibility data quality problem
patterns. The authors also found out that three underlying causes for users'
complaints regarding data not supporting their tasks are incomplete data, 
inadequately defined or measured data and data that could not be appropriately
aggregated.

The work of Kitchenham et al.\cite{kitchenham1996software} looks specifically at defining
quality in a software project, as well as finding who the people in charge of this 
are and how they should approach achieving it. They tried to define quality 
in software projects and analyze techniques that measure such metrics by looking
at other models proposed in different researches. However, they learned that quality is very hard to define and
there are various factors which need to be taken into consideration, such as the
business model of the company, the type of the software project (e.g. safety 
critical, financial sector) or the actors who are involved and how they coordinate
the software activities. 

Stamelos et al. \cite{stamelos2002code} tried to discuss and examine the quality
of the source code delivered by open source projects. They used a set of tools
that could automatically inspect various aspects of source code, looking at
a release of the OpenSUSE project and its components, written in C.
The results show that Linux applications have high quality 
code standards that one might expect in an open source repository, but 
the quality is lower than the one implied by the industry standards. More than half
of the components were in a high state of quality, but on the other hand, 
most lower quality components cannot be improved only by applying some
corrective actions. 

Having defined what data quality is and what are its characteristics, I needed
to research what are some methodologies that help measure it efficiently.
There are several research papers that present such methodologies, among which the 
work of \cite{lee2002aimq}. The authors tried to define an overall model along 
with an accompanying assessment instrument for quantifying information quality 
in an organization. The methodology has 3 main steps that need to be followed 
in order to apply it successfully:
  \begin{itemize}
    \item 2 $\times$ 2 model of what information quality means to managers;
    \item questionnaire for measuring information quality along the dimensions
    found in first step;
    \item two analysis techniques for interpreting the assessments captured 
    by the questionnaire.
  \end{itemize} 
In the end, the authors applied their proposed methodology to multiple organizations
and found out that it is practical and it could be used on a wider scale in the 
industry.

In contrast to the methodology proposed by \cite{lee2002aimq}, the solution found
by \cite{Heinrich2007MetricsDataQuality} is different - even though the 
main goals were the same, the authors used a single metric, and that is 
the metric for timeliness (i.e. whether the values of attributes still 
correspond to the current state of their real world counterparts and whether 
they are out of date). Thus, they applied the metric at a major German mobile 
services provider. Due to some Data Quality issues the company was having, they 
had lower mailing campaign success rates, but after applying the metrics, the 
company was able to establish a direct connection between the results of
measuring data quality and the success rates of campaigns.

%====================================================
\section{Ticket Quality}
\label{sec:ticket-quality}
%====================================================

Issue quality is an important topic of this literature review as it revolves around
software tickets and the characteristics of a well-written and 
informative bug report, how efficient are bug tracking systems, what defines
an efficient bug triaging process etc. 

The first and most important sub-topic of issue quality, which is the one
I will address in my own research as well, is the quality of bug reports.
There are several authors that have tried to defined what makes for a good bug
report and what components actually improve the overall quality.

\cite{bettenburg2008makes} analyzed what 
makes for a good bug report through qualitative analysis. They interviewed over
450 developers and asked them what are the most important features for them in
a bug report that help them solve the issue quicker. They reached the conclusion 
that stack traces and steps to reproduce increased the quality of a bug report 
the most, followed by well-written, grammar error free summaries and 
descriptions. Last but not least, they also created a tool called CueZilla that 
could with an accuracy rate between 31 and 48\% predict the quality of a bug 
report. Even though this paper is one of the very few that created a tool
similar to what my research revolves around, the industry has not been interested
in it mainly because it provides quite low levels of accuracy. Therefore, I am trying
to come up with a more comprehensive, but precise set of key elements that will increase the accuracy
levels considerably, which in turn might be of interest to software organizations.

Strengthening the argument that readability matters considerably in bug report 
quality is the work of \cite{hooimeijer2007modeling}. They ran an analysis over
27000 bug reports from the Mozilla Firefox project, looking at self-reported 
severity, readability, daily load, submitter reputation and changes over time. 
After running the evaluation, they not only found out about the importance of
readability in bug reports, but also that attachment and comment counts are 
valuable for faster triaging and that patch count, for example, does not
contribute in the same manner as the previous two.

Another research that agrees that stack traces are helpful in solving bug
reports faster is the one performed by \cite{schroter2010stack}. They conducted
the whole experiment on the Eclipse project. They first extracted the stack
traces using the infoZilla tool\cite{bettenburg2008extracting}, followed by
linking the stack traces to changes in the source code (change logs) by mining
the version repository of the Eclipse project. The results showed that 
around 60\% of the bugs that contained stack traces in their reports were fixed
in one of the methods in the frame, with 40\% being fixed in exactly the first
stack frame. Stacktraces are of key interest to my research as well, thus I will 
try to apply the methodology developed by the authors of this paper to my
own research. 

After examining the work of \cite{bettenburg2007quality}, we noticed that
the authors found very similar results to the ones presented in the research 
of \cite{bettenburg2008makes}. They performed the same type of evaluation method (i.e. 
interviews with developers) and confirmed that steps to reproduce and stack traces
are the most important features in a bug report that help developers solve the
issue quicker.

Other interesting findings regarding the quality of a software project's tickets
are the ones elicited in \cite{bettenburg2008duplicate}. The authors examined
whether duplicate bug reports are actually harmful to a project or they add 
quality. They collected big amount of data from the Eclipse project and ran
various kind of textual and statistical analysis on the data to find answers.
They reached the conclusion that bug duplicates contain information that is not
present in the master reports. This additional data can be helpful for developers
and it can also aid automated triaging techniques.

However, in order to model the quality of a bug report, one needs to be able to
successfully extract various types of information from such a report and, if 
possible, link them to the source code of the project (i.e. source code fragments
in bug report discussions should be linked to the corresponding sections in the 
actual code) or other software artifacts. There are several researches that tried 
to analyze such techniques and one of them is the work of 
\cite{bettenburg2012using}. The authors created a tool that could parse a bug 
report (using fuzzy code search) and extract source code that could then be
matched with exact locations in the source code, in the end producing a 
traceability link (i.e. tuple containing a clone group ID and file paths 
corresponding to the source code files). Evaluation showed an increase of
roughly 20\% in total traceability links between issue reports and source code 
when compared to the current state-of-the-art technique, change log analysis.

\cite{bettenburg2012using} also made use of a tool developed by 
\cite{bettenburg2008extracting} called infoZilla. This application can
parse bug reports and correctly extract patches, stack traces, source code and
enumerations. The authors evaluated it on the Eclipse project, where they saw
an accuracy level of over 97\% across more than 150,000 bug reports.

However, Kim et al. \cite{kim2013should} propose a rather different technique.
The authors employed a two-phase recommendation model that would locate the 
necessary fixes based on information in bug reports:
  \begin{itemize}
    \item firstly, they did a feature extraction on the bug reports (e.g. 
    extract description, summary, metadata);
    \item secondly, in order to successfully predict
    locations, this model was trained on the previously collected bug reports so that,
    when given a new bug report, it could automatically reveal where the necessary fixes
    needed to be applied;
    \item the actual implementation was put into place, and that
    is the two phase recommendation model, composed of binary (filters out 
    uninformative bug reports before predicting the files to fix) and multiclass (
    previously filtered bug reports are used as training data and only after that
    new bug reports can be analyzed and files to be changed recommended); the 
    overall accuracy was above the one achieved by \cite{bettenburg2012using}, 
    being able to rank over 70\%, but only as long as it recommended \emph{any} 
    locations.
  \end{itemize}

%====================================================
\section{Bug Report Modelling}
\label{sec:bug-report}
%====================================================

Bug report modelling based on context is the topic of my research and in this
section I will present the related work that has been previously published.

Just et al.\cite{just2008towards} investigated what are the flaws in 
modern issue tracking systems and what should be changed to make the development
process easier. The authors have asked almost 900 developers, of which
156 responded, various questions on what are the most important criterias
when designing an issue tracking system with efficiency in mind. They
applied card sort to rank the highest comments and they reached several
interesting conclusions:
  \begin{itemize}
    \item the most important elements in bug reports, regardless of the
    type of project or underlying technologies, are stacktraces, steps
    to reproduce and test cases;
    \item duplicated bug reports did not affect overall quality of the 
    development process, mainly because they provide additional information;
    \item even though reporters do know what is important to developers
    in a bug report, they did not always include that information when
    they filed the report; the main reason for this is that the reporters
    consider it is too difficult/time-consuming to gather such data.
  \end{itemize}

In contrast to the research of Just et al.\cite{just2008towards}, 
Zimmermann et al.\cite{zimmermann2009improving} adopted a more empirical
approach by looking at previous work in the field. The authors have
proposed several ideas to improve modern issue tracking systems, as well
as simulate their ideal issue tracking system where the user needs to
provide information based on a decision tree. The paper reaches the
main conclusion that users need to be asked the right questions in
bug reports based on the type of project and what the developers in the
team need most when fixing a bug. However, the authors also state that
their ideal system is not production-ready yet as it is in a preliminary
stage and it did not cover large amounts of data (i.e. 2,875 reports).

Another relevant research is the one of Breu et al.\cite{breu2010information},
where the authors try to improve the relationship between developers and
bug reporters. They have brought several major contributions to the community,
among which a \emph{catalogue of frequently asked questions}, split into
eight categories (e.g. missing information, clarification, triaging) and 
40 sub-categories; \emph{analysis of question time, response rate and time},
which revealed important findings such as correction questions are most 
likely to be answered quickly; bug reports are \emph{fixed faster if the
reporter is engaged in the process}; bug tracking systems should become
more \emph{user-centered} and allow developers engage with end users 
easier. This piece of research comes with conclusions which are very 
valuable to my work, especially as the authors have also published a 
technical report \cite{breu2009appendix} where they present all the findings.

A very important paper that is similar to my own research is the one 
published by Baysal et al.\cite{baysal2013situational}. The authors
have looked into how to personalize an issue tracking system based on the
developers working in the team, as well as the tasks they usually perform.
This work proposes a completely different approach than the research of
Breu et al.\cite{breu2010information} in that it favors a more 
developer-centric bug tracking system. The authors presented an enhanced
Bugzilla, which is one of the most widely used issue tracking systems, where
developers have the ability of tracking their own progress and also maintain
progress awareness across the whole project. I will try to develop my
own automatic contextual bug report modelling building on top of the findings
brought by this paper.



%====================================================
\section{Discussion and Conclusion}
\label{sec:conclusion}
%====================================================




\let\oldbibliography\thebibliography
\renewcommand{\thebibliography}[1]{\oldbibliography{#1}
\setlength{\itemsep}{-3pt}}

\bibliographystyle{abbrv}
%\setstretch{0.8}
{
\scriptsize
\bibliography{report}
}
\end{document}
